#!/usr/bin/env python3
"""Fetch jockey/coach statistics from Geny for a given course.

This script scrapes the Geny race page and extracts jockey and trainer
win percentages for each runner.  The resulting JSON is a mapping of
runner identifiers (as used in the Zeturf payload) to a small
``{"j_win": .., "e_win": ..}`` dictionary along with an overall
coverage percentage indicating how many runners were matched.

The collector expects the H-5 snapshot JSON generated by
``scripts/online_fetch_zeturf.py`` in order to map the scraped entries
back to the Zeturf identifiers.  When the optional ``GENY_COOKIE``
environment variable is defined the cookie is forwarded to the Geny
requests which can be required for some pages.
"""

from __future__ import annotations

import argparse
import json
import logging
import os
import re
import unicodedata
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, Iterator, List, Optional, Tuple

import requests
from bs4 import BeautifulSoup

LOGGER = logging.getLogger(__name__)
USER_AGENT = "Mozilla/5.0 (+EV; GPI JE Stats Collector)"
GENY_BASE_URL = "https://www.geny.com"
DEFAULT_TIMEOUT = 15


@dataclass
class RunnerStat:
    """Container for the statistics of a single runner."""

    num: str
    name: str
    j_win: Optional[float]
    e_win: Optional[float]

    def as_payload(self) -> Dict[str, float]:
        """Return a serialisable dictionary with defaulted numeric values."""

        payload: Dict[str, float] = {}
        if self.j_win is not None:
            payload["j_win"] = float(self.j_win)
        if self.e_win is not None:
            payload["e_win"] = float(self.e_win)
        return payload


def _normalize(text: str) -> str:
    """Return a case/diacritics insensitive representation of ``text``."""

    value = unicodedata.normalize("NFKD", text)
    value = "".join(ch for ch in value if ch.isalnum())
    return value.lower()


def _clean_number(text: str) -> Optional[str]:
    """Normalise a race number to its canonical string form."""

    if not text:
        return None
    match = re.search(r"\d+", text)
    if not match:
        return None
    num = match.group(0)
    num = num.lstrip("0") or "0"
    return num


def _parse_percentage(text: str) -> Optional[float]:
    """Extract a percentage value from ``text``.

    The function attempts to parse common formats encountered on Geny,
    such as ``"18 %"`` or ratios like ``"5/20"``.  Returned values are
    expressed in the ``0-100`` range.
    """

    if not text:
        return None

    cleaned = re.sub(r"\s+", " ", text)

    percent_match = re.search(r"(\d+(?:[.,]\d+)?)\s*%", cleaned)
    if percent_match:
        return float(percent_match.group(1).replace(",", "."))

    ratio_match = re.search(r"(\d+)\s*/\s*(\d+)", cleaned)
    if ratio_match:
        numerator = int(ratio_match.group(1))
        denominator = int(ratio_match.group(2))
        if denominator:
            return round(100.0 * numerator / denominator, 2)

    victory_match = re.search(r"victoires?\s*:\s*(\d+)", cleaned, re.IGNORECASE)
    if victory_match:
        return float(victory_match.group(1))

    return None


def extract_stats_from_table(soup: BeautifulSoup) -> List[RunnerStat]:
    """Parse the main runners table and extract jockey/coach stats."""

    stats: List[RunnerStat] = []
    for row in soup.find_all("tr"):
        cells = row.find_all("td")
        if len(cells) < 4:
            continue
        num = _clean_number(cells[0].get_text(strip=True))
        if num is None:
            continue
        name = cells[1].get_text(" ", strip=True)
        if not name:
            continue
        jockey_txt = cells[2].get_text(" ", strip=True)
        trainer_txt = cells[3].get_text(" ", strip=True)
        j_win = _parse_percentage(jockey_txt)
        e_win = _parse_percentage(trainer_txt)
        stats.append(RunnerStat(num=num, name=name, j_win=j_win, e_win=e_win))

    return stats


def _extract_json_blob(html: str) -> Optional[dict]:
    """Return the ``window.__NUXT__`` data structure if present."""

    match = re.search(r"window\.__NUXT__=({.+?});\s*</script>", html, flags=re.DOTALL)
    if not match:
        return None
    raw = match.group(1)
    raw = raw.replace("undefined", "null").replace("!0", "true").replace("!1", "false")
    try:
        return json.loads(raw)
    except json.JSONDecodeError:
        return None


def _iter_dicts(obj: object) -> Iterator[dict]:
    """Yield every dictionary nested inside ``obj`` recursively."""

    if isinstance(obj, dict):
        yield obj
        for value in obj.values():
            yield from _iter_dicts(value)
    elif isinstance(obj, list):
        for item in obj:
            yield from _iter_dicts(item)


def _extract_percentage_from_node(node: object) -> Optional[float]:
    """Search ``node`` for a plausible win percentage value."""

    if node is None:
        return None

    if isinstance(node, (int, float)):
        if 0 <= float(node) <= 1:
            return round(float(node) * 100, 2)
        if 0 <= float(node) <= 100:
            return float(node)
        return None

    if isinstance(node, str):
        return _parse_percentage(node)

    if isinstance(node, dict):
        for key, value in node.items():
            lower = key.lower()
            if isinstance(value, (int, float)):
                if (
                    "vict" in lower
                    or "win" in lower
                    or "ratio" in lower
                    or "taux" in lower
                ):
                    parsed = _extract_percentage_from_node(value)
                    if parsed is not None:
                        return parsed
            elif isinstance(value, str):
                parsed = _parse_percentage(value)
                if parsed is not None:
                    return parsed
            elif isinstance(value, (dict, list)):
                parsed = _extract_percentage_from_node(value)
                if parsed is not None:
                    return parsed
        return None

    if isinstance(node, list):
        for item in node:
            parsed = _extract_percentage_from_node(item)
            if parsed is not None:
                return parsed
        return None

    return None


def extract_stats_from_json(html: str) -> List[RunnerStat]:
    """Extract runner statistics from the embedded Nuxt payload."""

    blob = _extract_json_blob(html)
    if not blob:
        return []

    stats: List[RunnerStat] = []
    seen: set[Tuple[str, str]] = set()

    for node in _iter_dicts(blob):
        keys = {k.lower() for k in node.keys()}
        if not (
            {"horse", "jockey"}.issubset(keys) or {"horse", "trainer"}.issubset(keys)
        ):
            continue

        horse = node.get("horse") or {}
        if isinstance(horse, dict):
            name = (
                horse.get("name")
                or horse.get("nom")
                or node.get("horseName")
                or node.get("name")
            )
            number = (
                _clean_number(
                    str(
                        horse.get("number")
                        or horse.get("num")
                        or node.get("number")
                        or node.get("num")
                    )
                )
                if horse
                else None
            )
        else:
            name = node.get("name")
            number = (
                _clean_number(str(node.get("number") or node.get("num")))
                if node
                else None
            )

        if not name:
            continue

        key = (number or "", _normalize(name))
        if key in seen:
            continue
        seen.add(key)

        jockey = node.get("jockey") or node.get("driver") or node.get("pilote")
        trainer = node.get("trainer") or node.get("entraineur") or node.get("coach")

        j_win = _extract_percentage_from_node(jockey)
        e_win = _extract_percentage_from_node(trainer)

        stats.append(
            RunnerStat(
                num=number or "",
                name=str(name),
                j_win=j_win,
                e_win=e_win,
            )
        )

    return stats


def load_runner_index(path: Path | str | None) -> "RunnerIndex":
    """Load the H-5 snapshot and build a lookup index."""

    if path is None:
        return RunnerIndex([])

    payload = json.loads(Path(path).read_text(encoding="utf-8"))
    runners = payload.get("runners")
    if not isinstance(runners, list):
        runners = []
    return RunnerIndex(runners)


class RunnerIndex:
    """Mapping helper between runner numbers/names and identifiers."""

    def __init__(self, runners: Iterable[dict]):
        self.by_num: Dict[str, str] = {}
        self.by_name: Dict[str, List[str]] = {}
        self.ids: List[str] = []

        for runner in runners:
            rid = runner.get("id") or runner.get("ID") or runner.get("runner_id")
            if rid is None:
                continue
            rid_str = str(rid)
            self.ids.append(rid_str)

            num = (
                runner.get("num")
                or runner.get("number")
                or runner.get("startNumber")
                or runner.get("programmeNumber")
            )
            if num is not None:
                cleaned = _clean_number(str(num))
                if cleaned:
                    self.by_num.setdefault(cleaned, rid_str)

            name = runner.get("name") or runner.get("nom") or runner.get("horseName")
            if name:
                norm = _normalize(str(name))
                self.by_name.setdefault(norm, []).append(rid_str)

    @property
    def total(self) -> int:
        return len(self.ids)

    def lookup(self, stat: RunnerStat) -> Optional[str]:
        if stat.num:
            num = _clean_number(stat.num)
            if num and num in self.by_num:
                return self.by_num[num]

        norm = _normalize(stat.name)
        candidates = self.by_name.get(norm) or []
        if len(candidates) == 1:
            return candidates[0]
        return None


def map_stats_to_ids(
    stats: List[RunnerStat], index: RunnerIndex
) -> Tuple[float, Dict[str, Dict[str, float]], List[RunnerStat]]:
    """Map scraped statistics to Zeturf identifiers."""

    mapped: Dict[str, Dict[str, float]] = {}
    unmatched: List[RunnerStat] = []

    for stat in stats:
        rid = index.lookup(stat)
        if rid is None:
            unmatched.append(stat)
            continue
        payload = stat.as_payload()
        if not payload:
            unmatched.append(stat)
            continue
        mapped[rid] = payload

    coverage = 0.0
    if index.total:
        coverage = round(100.0 * len(mapped) / index.total, 2)

    return coverage, mapped, unmatched


def fetch_course_html(course_id: str, url: Optional[str] = None) -> str:
    """Download the Geny HTML page for ``course_id``."""

    headers = {
        "User-Agent": USER_AGENT,
        "Accept-Language": "fr-FR,fr;q=0.9,en;q=0.6",
    }
    cookie = os.getenv("GENY_COOKIE")
    if cookie:
        headers["Cookie"] = cookie

    if url is None:
        url = f"{GENY_BASE_URL}/partants-pmu/_c{course_id}"

    LOGGER.debug("Fetching Geny page %s", url)
    resp = requests.get(url, headers=headers, timeout=DEFAULT_TIMEOUT)
    resp.raise_for_status()
    return resp.text


def collect_stats(
    course_id: str, h5_path: Optional[str] = None, url: Optional[str] = None
) -> Tuple[float, Dict[str, Dict[str, float]]]:
    """Fetch and map jockey/trainer stats for ``course_id``."""

    html = fetch_course_html(course_id, url=url)
    soup = BeautifulSoup(html, "html.parser")

    stats = extract_stats_from_table(soup)
    if not stats:
        stats = extract_stats_from_json(html)

    index = load_runner_index(h5_path)
    coverage, mapped, unmatched = map_stats_to_ids(stats, index)

    if unmatched:
        LOGGER.warning(
            "Unmatched runners: %s",
            ", ".join(f"{s.num or '?'} {s.name}" for s in unmatched[:5]),
        )
    else:
        LOGGER.debug("All runners matched to identifiers")

    return coverage, mapped


def main() -> None:  # pragma: no cover - CLI glue
    parser = argparse.ArgumentParser(description="Fetch jockey/coach stats from Geny")
    parser.add_argument(
        "--course-id", required=True, help="Geny course identifier (numeric)"
    )
    parser.add_argument("--h5", help="Path to the H-5 snapshot JSON for id mapping")
    parser.add_argument("--out", required=True, help="Destination JSON file")
    parser.add_argument("--url", help="Override Geny URL (for debugging)")
    parser.add_argument("--log-level", default="INFO", help="Logging level")
    args = parser.parse_args()

    logging.basicConfig(level=getattr(logging, args.log_level.upper(), logging.INFO))

    coverage, mapped = collect_stats(args.course_id, h5_path=args.h5, url=args.url)

    payload: Dict[str, object] = {"coverage": coverage}
    payload.update(mapped)

    out_path = Path(args.out)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_text(
        json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8"
    )
    LOGGER.info(
        "Wrote %d entries to %s (coverage %.2f%%)", len(mapped), out_path, coverage
    )


if __name__ == "__main__":  # pragma: no cover
    main()


def enrich_from_snapshot(snapshot_path: str, reunion: str = "", course: str = "") -> str:
    """
    Lit le JSON H-5, produit <stem>_je.csv (cache activé).
    Retourne le chemin du CSV.
    """
    import subprocess, shlex
    from pathlib import Path
    sp = Path(snapshot_path)
    out = sp.with_name(f"{sp.stem}_je.csv")
    cmd = f'python scripts/fetch_je_stats.py --h5 "{sp}" --out "{out}" --cache --ttl-seconds 86400'
    subprocess.run(shlex.split(cmd), check=True)
    return str(out)
