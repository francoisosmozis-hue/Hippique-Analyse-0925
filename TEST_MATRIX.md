# Matrice de Tests - Projet Hippique Orchestrator

Ce document détaille la stratégie de test pour chaque composant majeur du projet. Il identifie les risques, les tests existants, les tests à ajouter, et les indicateurs de succès.

| Composant | Risque Associé | Tests Existants (Couverture) | Tests Manquants | KPI de Succès | Effort | Priorité |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **1. API Publique & UI** | Vulnérabilités, régression du schéma de données, expérience utilisateur dégradée. | `tests/test_api_endpoints.py` (bons), `tests/test_service.py` (partiels). Couverture de `service.py` : 76%. | - Test d'intégration validant le schéma JSON complet de `/api/pronostics`.<br>- Test UI validant la présence de marqueurs HTML clés et l'appel JS à l'API. | Schéma JSON stable (validé par test).<br>Le endpoint UI contient `div#pronostics-container`. | Faible | **Haute** |
| **2. Endpoints Sensibles** | Accès non autorisé à des fonctions critiques (scheduling, runs manuels). | `tests/test_api_security.py` : bonne base (API Key, OIDC). | - Test explicite : `/schedule` retourne 403 sans `X-API-KEY`.<br>- Test explicite : `/ops/run` retourne 403 si `REQUIRE_AUTH=true` et pas d'authentification. | 100% des endpoints sensibles sont testés contre l'accès non autorisé. | Faible | **Haute** |
| **3. Pipeline d'Analyse** | Logique de décision de pari incorrecte, erreurs non gérées. | `tests/test_pipeline_run.py`, `tests/test_analysis_pipeline.py` (Couv: `analysis_pipeline.py` 99%, `pipeline_run.py` 80%). | - Tests sur `pipeline_run.py` pour les cas limites (ROI très faible/élevé, données manquantes).<br>- Tests sur les fonctions de `analysis_utils.py` peu couvertes. | Couverture `pipeline_run.py` > 90%.<br>Couverture `analysis_utils.py` > 95%. | Moyen | Moyenne |
| **4. Persistance (Firestore & GCS)** | Corruption de données, erreurs de lecture/écriture, perte de données. | `tests/test_firestore_client.py` (95%), `tests/test_gcs_client.py`. Couverture `gcs_utils.py` : 0%. | - **`firestore_client.py`**: Test sur collection vide, test de format de `doc_id`.<br>- **`gcs_utils.py`**: Tests unitaires pour chaque fonction (read/write/list).<br>- **`gcs_client.py`**: Augmenter la couverture des cas d'erreur. | Couverture `firestore_client.py` > 98%.<br>Couverture `gcs_utils.py` > 80%. | Moyen | **Critique** |
| **5. Scrapers (Boturfers, Zeturf)** | Erreurs de parsing silencieuses, changements de structure non détectés. | `tests/test_scraper_boturfers.py` (87%), `tests/test_scraper_zeturf.py` (100%). | - **Fixtures HTML** : Ajouter des fixtures pour tous les cas (nominal, partants manquants, etc.).<br>- **Tests de parsing** : Valider le contenu extrait (nombre de partants, cote) pour chaque fixture.<br>- **Test "canary"** : Ajouter un test qui échoue si la structure de la page (ex: `div#main`) change. | >5 fixtures HTML par scraper.<br>100% de la logique de parsing est validée via fixtures. | Moyen | **Haute** |
| **6. Scheduler (Cloud Tasks)** | Tâches non créées, erreurs de configuration (OIDC, URL). | `tests/test_scheduler.py` (100%), `tests/test_scheduler_extended.py`. | - Tests des cas d'erreur dans `enqueue_run_task` (URL de service manquante, etc.).<br>- Valider la charge utile (payload) de la tâche créée. | Couverture `scheduler.py` maintenue à 100% avec tests d'erreur. | Faible | Moyenne |
| **7. Gestion de l'Environnement** | Démarrage en production avec une configuration incomplète masquant des erreurs. | `tests/test_env_utils.py`. Couverture `config/env_utils.py` à vérifier. | - Test qui prouve que `get_env` retourne la valeur par défaut sans erreur si `IS_PROD=False`.<br>- Test qui prouve que `get_env` lève une exception (`fail-fast`) si `IS_PROD=True` et une variable requise est manquante. | Couverture `config/env_utils.py` > 95%. Le comportement "fail-fast" est testé. | Faible | **Critique** |