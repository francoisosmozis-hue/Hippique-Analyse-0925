# Matrice de Tests - Hippique Orchestrator

Ce document détaille la stratégie de test par composant, en évaluant les risques et en identifiant les tests manquants.

| Composant | Risque Technique / Métier | Tests Existants (Couverture) | Tests Manquants | KPI de Succès | Effort | Priorité |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **API Publique** (`/api/pronostics`) | Stabilité du contrat de données, performance sous charge modérée. | Tests unitaires sur le schéma de réponse, le code de statut, la gestion des ETag. | - Test d'intégration validant le JSON avec `jsonschema`.<br>- Test de non-régression sur un payload de référence. | 100% des réponses valident le schéma JSON de référence. | Faible | Haute |
| **UI** (`/pronostics`) | Disponibilité de base, non-régression de l'interface. | Test unitaire simple vérifiant le code de statut 200. | - Test vérifiant la présence d'un marqueur HTML clé (ex: `id="pronostics-container"`).<br>- Test vérifiant que l'attribut `data-api-endpoint` est correct. | La page se charge et contient les marqueurs HTML attendus. | Faible | Moyenne |
| **Endpoints Sensibles** (`/schedule`, `/ops/*`, `/tasks/*`) | Violation de la sécurité (accès non autorisé). | Tests unitaires vérifiant les codes 401/403 en l'absence de clé API / token OIDC. | - Scénario de test d'intégration `TestClient` pour `/schedule` avec et sans clé API.<br>- Protocole de test manuel pour le `smoke_prod.sh`. | 100% des endpoints sensibles rejettent les accès non authentifiés. | Faible | **Critique** |
| **Pipeline d'Analyse** (`analysis_pipeline.py`, `pipeline_run.py`) | Erreurs de logique, gestion incorrecte des cas limites (ex: pas de données, ROI faible). | Excellente couverture (99%, 80%). Tests sur les cas d'abstention, erreurs GCS, etc. | - Augmenter la couverture de `pipeline_run.py` > 90% en ciblant les branches non couvertes.<br>- Ajouter des tests de cas limites (ex: `runners` avec données manquantes). | Couverture `pipeline_run.py` > 90%. | Moyen | **Haute** |
| **Persistance** (`firestore_client.py`, `gcs_client.py`) | Intégrité des données, gestion des erreurs de connexion. | Très bonne couverture (95%, 75%). Tests sur les exceptions, la création de `doc_id`. | - Augmenter la couverture de `firestore_client.py` > 95% (déjà atteint).<br>- Ajouter des tests pour les cas "collection vide" ou "document non trouvé".<br>- **Augmenter la couverture de `gcs_client.py` > 85%**. | Couverture > 90% pour `firestore_client` et `gcs_client`. | Faible-Moyen | Moyenne |
| **Scrapers** (`scrapers/`) | Changement de structure HTML des sites sources, parsing incorrect. | Bonne couverture (87-100%). Tests unitaires sur des fixtures HTML locales. | - Ajouter une fixture HTML pour chaque scraper simulant un changement de structure (ex: classe CSS modifiée) et vérifier la levée d'une erreur contrôlée.<br>- Documenter un protocole de "canary testing" pour surveiller les changements en production. | Les tests de "rupture de contrat HTML" échouent comme attendu. | Moyen | Moyenne |
| **Scheduler & Cloud Tasks** (`scheduler.py`) | Erreurs de planification (fuseau horaire, heure passée), erreurs de création de tâches. | Excellente couverture (100%). Tests sur la planification (dry run, force), gestion des erreurs (permissions, etc.). | - Test d'intégration `TestClient` simulant un appel à `/schedule` qui vérifie le nombre et le payload des tâches qui *seraient* créées (via mock). | Le mock du client Cloud Tasks est appelé avec le bon nombre de tâches et les bons payloads. | Faible | Haute |
| **Gestion de l'Environnement** (`config/env_utils.py`) | Mauvaise configuration silencieuse en production (pas de "fail-fast"). | Couverture inconnue. Tests existants sur les valeurs par défaut, les alias et les erreurs de casting. | - **Mesurer la couverture actuelle.**<br>- Ajouter un test spécifique pour `get_env` avec `is_prod=True` et une variable requise manquante, vérifiant que `sys.exit(1)` est appelé. | Couverture > 95%. Le mode "fail-fast" en production est testé et validé. | Faible | **Critique** |
