"""Utilities to build a calibration dataset for :math:`p_true`.

The historical data exported by the analysis pipeline is stored under
``data/RxCy`` folders.  Each race directory may contain the following
artefacts:

* ``per_horse_report.csv`` – runner level statistics captured at H-30 and
  H-5 (odds, jockey/trainer wins, etc.).
* ``p_finale.json`` – tickets generated by the pipeline.
* ``arrivee_officielle.json`` – official arrival once the race is finished.

This module extracts a tabular dataset combining those sources.  It is
designed to be lightweight and resilient to partial data so the unit tests can
exercise the logic using synthetic fixtures.

The resulting dataframe contains the features required by the logistic
calibration model implemented in :mod:`calibration.p_true_model`.
"""

from __future__ import annotations

import datetime as dt
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Iterable, List

import lightgbm as lgb
import pandas as pd
import yaml

from sklearn.metrics import brier_score_loss, log_loss


@dataclass
class CalibrationResult:
    model: Any
    features: List[str]
    n_samples: int
    n_races: int
    brier_score: float
    log_loss: float
    fitted_at: dt.datetime


# ... (le reste des imports et des fonctions jusqu'à train_logistic_model)


def train_logistic_model(
    dataset: pd.DataFrame,
    *,
    features: Iterable[str],
    C: float = 1.0,  # Note: C is for LogisticRegression, will be adapted for LGBM
    random_state: int = 42,
) -> CalibrationResult:
    """Fit a LightGBM classifier and return diagnostics."""

    df = dataset.dropna(subset=["is_winner"])
    if df.empty:
        raise ValueError("dataset must contain at least one labelled sample")

    feature_list = [str(f) for f in features]
    X = df[feature_list].to_numpy()
    y = df["is_winner"].astype(int).to_numpy()

    # Utilisation de LightGBM au lieu de la régression logistique
    model = lgb.LGBMClassifier(
        objective="binary",
        metric="logloss",
        random_state=random_state,
        n_estimators=100,  # Peut être optimisé par cross-validation
        learning_rate=0.05,  # Peut être optimisé
        num_leaves=31,  # Peut être optimisé
    )
    model.fit(X, y)

    proba = model.predict_proba(X)[:, 1]
    brier = float(brier_score_loss(y, proba))
    loss = float(log_loss(y, proba, labels=[0, 1]))

    return CalibrationResult(
        model=model,  # Le modèle est maintenant un LGBMClassifier
        features=feature_list,
        n_samples=int(len(df)),
        n_races=int(df["race_id"].nunique() if "race_id" in df else 0),
        brier_score=brier,
        log_loss=loss,
        fitted_at=dt.datetime.now(dt.timezone.utc),
    )


def serialize_model(result: CalibrationResult, path: Path, *, C: float = 1.0) -> None:
    """Write ``result`` to ``path`` using a joblib dump for the model."""

    # LightGBM models are best saved with joblib or their own format
    import joblib

    model_path = path.with_suffix(".joblib")
    joblib.dump(result.model, model_path)

    payload = {
        "version": 2,  # Version du modèle incrémentée
        "model_format": "lightgbm_joblib",
        "model_path": str(model_path.name),
        "features": result.features,
        "metadata": {
            "n_samples": result.n_samples,
            "n_races": result.n_races,
            "brier_score": result.brier_score,
            "log_loss": result.log_loss,
            "fitted_at": result.fitted_at.isoformat().replace("+00:00", "Z"),
        },
    }

    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(
        yaml.safe_dump(payload, sort_keys=False, allow_unicode=True),
        encoding="utf-8",
    )
